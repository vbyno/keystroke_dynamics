{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tabulate\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataTransformerRegistry.enable('default')"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "alt.data_transformers.enable('default', max_rows=None) \n",
    "# alt.renderers.enable('default')\n",
    "# alt.renderers.enable('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataTransformerRegistry.enable('custom')"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Save temp altair json files in separate folder\n",
    "os.makedirs('tmp/altdata', exist_ok=True)\n",
    "\n",
    "def custom(data):\n",
    "    return alt.pipe(data, alt.to_json(filename='tmp/altdata/{prefix}-{hash}.{extension}') )\n",
    "\n",
    "alt.data_transformers.register('custom', custom)\n",
    "alt.data_transformers.enable('custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_datasets(datasets):\n",
    "    return pd.concat(\n",
    "        (dataset[[\n",
    "            'UserName', \n",
    "            'AccessKey', \n",
    "            'Topic', \n",
    "            'Task',\n",
    "            'ReviewText', \n",
    "            'ReviewMeta'\n",
    "        ]] for dataset in datasets),\n",
    "        ignore_index=True)\n",
    "\n",
    "def read_dataset(files = ('data/ReviewAMT_500_t.csv', 'data/GayMarriage_400.csv', 'data/GunControl_400.csv')):\n",
    "    df_atm = pd.read_csv(files[0], sep='\\t')\n",
    "    df_gay = pd.read_csv(files[1], sep='\\t')\n",
    "    df_gun = pd.read_csv(files[2], sep='\\t')\n",
    "\n",
    "    df_atm.rename(columns = {'ReviewTopic': 'Topic'}, inplace=True)\n",
    "    df_atm['Task'] = df_atm['Task'].map(\n",
    "        {\n",
    "            \"Fake Review\": 'fake', \n",
    "            \"True Review\": 'true',\n",
    "            'Copy_1': 'copy_1',\n",
    "            'Copy_2': 'copy_2'\n",
    "        })\n",
    "    df_gay['Task'] = df_gay['Task'].map(\n",
    "        {\n",
    "            \"Fake Essay\": 'fake', \n",
    "            \"True Essay\": 'true',\n",
    "            'Copy_1': 'copy_1',\n",
    "            'Copy_2': 'copy_2'\n",
    "        }) \n",
    "    df_gun['Task'] = df_gun['Task'].map(\n",
    "        {\n",
    "            \"Fake Essay\": 'fake', \n",
    "            \"True Essay\": 'true',\n",
    "            'Copy_1': 'copy_1',\n",
    "            'Copy_2': 'copy_2'\n",
    "        }) \n",
    "    return concat_datasets((df_atm, df_gay, df_gun))\n",
    "\n",
    "df = read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5200 entries, 0 to 5199\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   UserName    5200 non-null   object\n 1   AccessKey   5200 non-null   object\n 2   Topic       5200 non-null   object\n 3   Task        5200 non-null   object\n 4   ReviewText  5200 non-null   object\n 5   ReviewMeta  5200 non-null   object\ndtypes: object(6)\nmemory usage: 243.9+ KB\n"
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                          ReviewText  \\\n0  Linear algebra is central to almost all areas ...   \n1  Linear algebra is central to almost all areas ...   \n2  Linear algebra is central to almost all areas ...   \n\n                                          ReviewMeta  \\\n0  1582990688239 KeyDown 16;1582990688600 KeyDown...   \n1  1582996141506 KeyDown 16;1582996141595 KeyDown...   \n2  1582996489340 KeyDown 16;1582996489541 KeyDown...   \n\n                              AccessKey                              UserName  \\\n0  9d891be4-e43e-49f9-88bb-25314e670850  9d891be4-e43e-49f9-88bb-25314e670850   \n1  9d891be4-e43e-49f9-88bb-25314e670850  9d891be4-e43e-49f9-88bb-25314e670850   \n2  97fc22cf-aa61-4dcf-91a8-97d8fbf4a983  97fc22cf-aa61-4dcf-91a8-97d8fbf4a983   \n\n     Task Topic  \n0  copy_1    LA  \n1  copy_2    LA  \n2  copy_1    LA  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ReviewText</th>\n      <th>ReviewMeta</th>\n      <th>AccessKey</th>\n      <th>UserName</th>\n      <th>Task</th>\n      <th>Topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Linear algebra is central to almost all areas ...</td>\n      <td>1582990688239 KeyDown 16;1582990688600 KeyDown...</td>\n      <td>9d891be4-e43e-49f9-88bb-25314e670850</td>\n      <td>9d891be4-e43e-49f9-88bb-25314e670850</td>\n      <td>copy_1</td>\n      <td>LA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Linear algebra is central to almost all areas ...</td>\n      <td>1582996141506 KeyDown 16;1582996141595 KeyDown...</td>\n      <td>9d891be4-e43e-49f9-88bb-25314e670850</td>\n      <td>9d891be4-e43e-49f9-88bb-25314e670850</td>\n      <td>copy_2</td>\n      <td>LA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Linear algebra is central to almost all areas ...</td>\n      <td>1582996489340 KeyDown 16;1582996489541 KeyDown...</td>\n      <td>97fc22cf-aa61-4dcf-91a8-97d8fbf4a983</td>\n      <td>97fc22cf-aa61-4dcf-91a8-97d8fbf4a983</td>\n      <td>copy_1</td>\n      <td>LA</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "def read_group_sessions(name='data/keystroke_sessions-2020-03-20.csv'):\n",
    "    _a = pd.read_csv(name)\n",
    "    # _a.rename(columns={\"AccessKey\": \"UserName\", }\n",
    "    _a['UserName'] = _a['AccessKey']\n",
    "    _a.drop(columns=['attempt_id', 'ReviewDate'], inplace=True)\n",
    "    _a['Task'] = ''\n",
    "    _a['Topic'] = 'LA'\n",
    "    attempts = {}\n",
    "    for i, row in _a.iterrows():\n",
    "        key = row['UserName']\n",
    "        if key not in attempts: attempts[key] = 0\n",
    "        attempts[key] += 1\n",
    "        _a.loc[i, 'Task'] = f\"copy_{attempts[key]}\"\n",
    "\n",
    "    return _a\n",
    "\n",
    "def add_text_length_column(df):\n",
    "    df['TextLenght'] = df.apply(lambda x: len(x['ReviewText'].split()), axis = 1)\n",
    "\n",
    "df_sessions = read_group_sessions()\n",
    "df_sessions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_keystroke(line):\n",
    "    res = line.split(' ')\n",
    "    if len(res) > 3:\n",
    "        return f'UNKNOWN: {line}'\n",
    "    \n",
    "    time, command, key = res\n",
    "    return (time, command, key, chr(int(key)))\n",
    "\n",
    "def code_to_str(keycode):\n",
    "    keycode = int(keycode)\n",
    "    mappings = {\n",
    "        16: 'shift',\n",
    "        8: 'backspace',\n",
    "        32: 'space',\n",
    "        188: 'comma',\n",
    "        190: 'dot'\n",
    "    }\n",
    "    return mappings.get(keycode, chr(keycode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque \n",
    "\n",
    "def process_keystrokes(inputs, humanize=False, distinguish_shift=True, small_transitions=True):      \n",
    "    MOUSE_UP = 'MouseUp'\n",
    "    KEY_DOWN = 'KeyDown'\n",
    "    KEY_UP = 'KeyUp'\n",
    "    TRANSITION_2 = 'transition_2'\n",
    "    dwells = {}\n",
    "    transitions_1 = {}\n",
    "    transitions_2 = {}\n",
    "    key_downs = {}\n",
    "    keys_queue = deque([])\n",
    "    last_key_up = None\n",
    "\n",
    "    def correct_transitions():\n",
    "        if small_transitions:\n",
    "            return transitions_2\n",
    "        else:\n",
    "            return transitions_1\n",
    "    \n",
    "    def record_key(code1, code2, value, collection):\n",
    "        key = \"\"\n",
    "        if (16 in key_downs or last_key_up == 16) and code1 != 16 and distinguish_shift:\n",
    "            key = key + \"[shift]\"\n",
    "        if code2:\n",
    "            key = key + f\"{code1}_{code2}\"\n",
    "        else:\n",
    "            key = key + f\"{code1}\"\n",
    "\n",
    "        if key not in collection: \n",
    "            collection[key] = []\n",
    "        collection[key].append(value)\n",
    "\n",
    "    for keystroke in inputs.split(';'):\n",
    "        res = keystroke.split(' ')\n",
    "        \n",
    "        if len(res) < 3: continue  \n",
    "        if res[1] == 'MouseUp': continue\n",
    "\n",
    "        time, command, code = res\n",
    "        time = int(time)\n",
    "        code = int(code)\n",
    "        \n",
    "        if command == KEY_DOWN:\n",
    "            if keys_queue:\n",
    "                prev_code, prev_time_down, prev_time_up = keys_queue[0]\n",
    "\n",
    "                if prev_time_up: \n",
    "                    record_key(prev_code, code, time - prev_time_up, transitions_2)\n",
    "\n",
    "                record_key(prev_code, code, time - prev_time_down, transitions_1)\n",
    "            \n",
    "            key_downs[code] = time\n",
    "            keys_queue.appendleft([code, time, None])\n",
    "            \n",
    "        if command == KEY_UP:\n",
    "            following_key = None\n",
    "            for i_key in keys_queue:\n",
    "                if i_key[0] == code:\n",
    "                    i_key[2] = time\n",
    "                    record_key(code, None, time - i_key[1], dwells)\n",
    "                    \n",
    "                    if following_key and following_key[1] < i_key[2]:\n",
    "                        record_key(i_key[0], following_key[0], following_key[1] - i_key[2], transitions_2)\n",
    "                    break\n",
    "\n",
    "                following_key = i_key\n",
    "\n",
    "            if code in key_downs: del(key_downs[code])\n",
    "            last_key_up = code\n",
    "\n",
    "    if humanize:\n",
    "        new_dwells = {}\n",
    "        new_transitions = {}\n",
    "        shift_h = \"[shift]\"\n",
    "\n",
    "        for key, inputs in dwells.items():\n",
    "            prefix = \"\"\n",
    "            if shift_h in key:\n",
    "                key = key[7:]\n",
    "                prefix = shift_h\n",
    "            new_key = prefix + f\"{code_to_str(int(key))}\"\n",
    "            new_dwells[new_key] = sorted(inputs)\n",
    "\n",
    "        for key, inputs in correct_transitions().items():\n",
    "            try:\n",
    "                k1, k2 = key.split(\"_\")\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            prefix = \"\"\n",
    "            if shift_h in k1:\n",
    "                k1 = k1[7:]\n",
    "                prefix = shift_h\n",
    "\n",
    "            key = f\"{prefix}{code_to_str(int(k1))}_{code_to_str(int(k2))}\"\n",
    "            new_transitions[key] = sorted(inputs)\n",
    "\n",
    "        return new_dwells, new_transitions\n",
    "\n",
    "    return dwells, correct_transitions()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'A': [127, 151], '[shift]H': [135], '[shift]E': [152], '[shift]L': [110, 127], 'shift': [2600, 3400], '[shift]O': [159]}\n{'A_A': [633], 'A_shift': [592], '[shift]H_E': [632], '[shift]E_L': [345], '[shift]L_L': [178], 'shift_H': [-2895], '[shift]L_shift': [1968], 'shift_O': [-1879]}\n"
    }
   ],
   "source": [
    "ss = \"1583128026026 KeyDown 65;1583128026177 KeyUp 65;1583128026810 KeyDown 65;1583128026937 KeyUp 65;1583128027529 KeyDown 16;1583128028034 KeyDown 72;1583128028169 KeyUp 72;1583128028801 KeyDown 69;1583128028953 KeyUp 69;1583128029298 KeyDown 76;1583128029408 KeyUp 76;1583128029586 KeyDown 76;1583128029713 KeyUp 76;1583128030929 KeyUp 16;1583128031681 KeyDown 16;1583128032402 KeyDown 79;1583128032561 KeyUp 79;1583128034281 KeyUp 16\"\n",
    "\n",
    "dwells, transitions = process_keystrokes(ss, humanize=True, small_transitions=True)\n",
    "print(dwells)\n",
    "print(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 832 entries, 0 to 5196\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   UserName    832 non-null    object\n 1   AccessKey   832 non-null    object\n 2   Topic       832 non-null    object\n 3   Task        832 non-null    object\n 4   ReviewText  832 non-null    object\n 5   ReviewMeta  832 non-null    object\ndtypes: object(6)\nmemory usage: 45.5+ KB\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def split_datasets(df):\n",
    "#     df1 = df.loc[df['Task'] == 'copy_1']\n",
    "#     df1.drop_duplicates(subset =\"UserName\", keep = False, inplace = True) \n",
    "\n",
    "#     df2 = df.loc[df['Task'] == 'copy_2']\n",
    "#     df2.drop_duplicates(subset =\"UserName\", keep = False, inplace = True) \n",
    "\n",
    "#     train_X, val_X = train_test_split(df1, test_size=0.2, random_state=42)\n",
    "#     train_y = df2[df2['UserName'].isin(train_X['UserName'].tolist())]\n",
    "#     val_y = df2[df2['UserName'].isin(val_X['UserName'].tolist())]\n",
    "#     return train_X, val_X, train_y, val_y\n",
    "\n",
    "def split_datasets(df):\n",
    "    df_new = df.copy()\n",
    "    df_new = df_new.drop_duplicates(subset =[\"UserName\", \"Task\"], keep = False, inplace = False) \n",
    "    test_column = 'copy_2'\n",
    "    # df1 = df_new.loc[df['Task'] != test_column]\n",
    "    df1 = df_new.loc[df['Task'] == 'copy_1']\n",
    "    df2 = df_new.loc[df['Task'] == test_column]\n",
    "    return df1, None, df2, None\n",
    "\n",
    "# df_train, df_val, train_y, val_y = split_datasets(concat_datasets((df, df_sessions)))\n",
    "df_train, df_val, train_y, val_y = split_datasets(df)\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['space', 'E', 'T', 'A', 'O', 'backspace', 'N', 'I', 'S', 'R', 'H',\n",
    "       'backspace_backspace', 'L', 'E_space', 'D', 'space_T', 'U', 'C',\n",
    "       'space_A', 'T_H', 'S_space', 'M', 'shift', 'G', 'Y', 'T_space',\n",
    "       'F', 'H_E', 'W', 'P', 'D_space', 'A_N', 'I_N', 'R_E', 'N_space',\n",
    "       'Y_space', 'E_R', 'B', 'space_shift', 'space_I', 'space_O',\n",
    "       'space_W', 'V', 'dot', 'space_S', 'O_space', 'O_N', 'N_D', 'E_N',\n",
    "       'R_space', 'dot_space', 'A_T', 'O_U', 'T_O', 'space_C', 'I_T',\n",
    "       'space_B', 'A_R', 'E_S', 'H_A', 'I_S', 'V_E', 'comma', 'A_L',\n",
    "       'space_M', 'space_F', 'comma_space', 'R_I', 'O_R',\n",
    "       'space_backspace', 'L_E', 'S_T', 'N_T', 'N_G', 'space_P',\n",
    "       'space_H', 'S_E', 'T_I', 'A_space', 'M_E', 'space_G', 'T_E', 'E_D',\n",
    "       'F_space', 'L_space', 'E_A', 'K', 'A_S', 'B_E', 'M_A', 'L_L',\n",
    "       'G_space', '[shift]space', '[shift]I', 'space_R', 'space_D', 'U_N',\n",
    "       'O_F', 'H_O', 'H_I', 'C_O', 'shift_I', 'N_E', 'space_L', 'R_O',\n",
    "       'space_N', 'O_M', 'O_T', 'L_I', 'space_E', 'D_E', '[shift]T',\n",
    "       'N_S', 'I_C', 'C_E', 'L_Y', 'T_A', 'G_E', 'I_O', 'U_R', 'E_L',\n",
    "       'F_O', 'U_S', 'N_O', 'P_E', 'H_space', 'S_O', 'I_L', 'W_E',\n",
    "       'shift_T', 'C_H', 'C_A', 'P_L', 'U_L', 'E_T', 'W_A', 'A_Y', 'T_R',\n",
    "       'L_D', 'U_T', 'L_A', 'R_A', 'E_C', 'E_E', 'I_E', 'W_I',\n",
    "       '[shift]space_shift', 'backspace_space', 'G_U', 'W_H', 'S_H',\n",
    "       'A_V', 'A_C', 'E_backspace', 'O_O', 'space_space', 'O_P', 'R_Y',\n",
    "       'O_W', 'I_A', 'R_R', 'Þ', 'L_O', 'S_I', 'O_L', 'A_G', 'D_I', 'I_G',\n",
    "       'E_V', 'A_M', 'G_H', 'W_O', 'Y_O', 'M_O', 'backspace_shift', 'O_S',\n",
    "       'G_A', 'P_O', 'K_E', 'R_S', 'backspace_E', 'P_R', 'S_dot', 'E_dot',\n",
    "       'I_M', 'I_R', 'M_space', 'S_A', 'backspace_T', 'backspace_A', 'X',\n",
    "       'C_I', 'space_Y', 'E_Y', 'F_E', 'O_D', 'D_O', 'space_U', 'T_S',\n",
    "       'A_D']\n",
    "\n",
    "USE_TRANSITION_2 = False\n",
    "N_NEIGHBORS=1\n",
    "FEATURES_COUNTER = {}\n",
    "TRACK_FEATURES_COUNT = False\n",
    "NORMALIZATOR_COEF=1.5\n",
    "DISTINGUISH_SHIFT=False\n",
    "USE_MEAN = True \n",
    "USE_MEDIAN = False\n",
    "USE_STD = False\n",
    "USE_VARIANCE = False \n",
    "DISTANCE_MEASURE = 2 # 1 - manhattan, 2 - euql.\n",
    "NORMALIZATOR_ORIGINAL_MEDIAN = True\n",
    "from sklearn import impute, preprocessing\n",
    "DATA_IMPUTER = impute.SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# DATA_TRANSFORMER = preprocessing.MaxAbsScaler(), preprocessing.RobustScaler(), preprocessing.Normalizer(), preprocessing.MinMaxScaler(), preprocessing.StandardScaler()\n",
    "DATA_TRANSFORMER = preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_raw_data_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_values(array, coef=1.5, original_median=True):\n",
    "    array = np.array(array)\n",
    "    q1 = np.quantile(array, 0.25)\n",
    "    q3 = np.quantile(array, 0.75)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    if NORMALIZATOR_ORIGINAL_MEDIAN: median = np.median(array)\n",
    "    array = array[(array > (q1 - coef * iqr)) & (array < (q3 + coef * iqr))]\n",
    "    if not NORMALIZATOR_ORIGINAL_MEDIAN: median = np.median(array)\n",
    "\n",
    "    return array.mean(), median, array.std(), array.var()\n",
    "\n",
    "def process_and_unite_keystrokes(inputs, index, features=FEATURES): \n",
    "    humanize = True\n",
    "    if index not in cached_raw_data_dict:\n",
    "        a,b = process_keystrokes(inputs, humanize, DISTINGUISH_SHIFT, small_transitions=USE_TRANSITION_2)\n",
    "        raw_data = {**a , **b}\n",
    "        cached_raw_data_dict[index]=raw_data \n",
    "    else: \n",
    "        raw_data = cached_raw_data_dict[index]\n",
    "    res = {}\n",
    "\n",
    "    for feature in features:\n",
    "        if feature in raw_data: \n",
    "            mean, median, std, var = timestamp_values(raw_data[feature], NORMALIZATOR_COEF)\n",
    "        else: \n",
    "            mean, median, std, var = None, None, None, None\n",
    "\n",
    "        if USE_MEAN: res[f\"{feature}_mean\"] = mean \n",
    "        if USE_MEDIAN: res[f\"{feature}_median\"] = median \n",
    "        if USE_STD: res[f\"{feature}_std\"] = std \n",
    "        if USE_VARIANCE: res[f\"{feature}_var\"] = var\n",
    "\n",
    "    if TRACK_FEATURES_COUNT:\n",
    "        # Save statistics of appearance of each key\n",
    "        for key, timestamps in raw_data.items():\n",
    "            if key not in FEATURES_COUNTER: FEATURES_COUNTER[key] = 0\n",
    "            FEATURES_COUNTER[key] += len(timestamps)\n",
    "\n",
    "    return res\n",
    "\n",
    "def mutate_dataset(df, \n",
    "    user_id_column = \"UserName\", \n",
    "    keystrokes_column=\"ReviewMeta\", \n",
    "    transition_2=False,\n",
    "    features=FEATURES):\n",
    "    res = []\n",
    "    for index, inputs in df.iterrows():\n",
    "        data = process_and_unite_keystrokes(inputs[keystrokes_column], index, features)\n",
    "        res.append({'user_id': inputs[user_id_column], **data})\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "def to_pandas_dataframe(res_copy, res, y_column='user_id'):\n",
    "    res_copy = pd.DataFrame(res_copy, columns = res.columns.difference([y_column]))\n",
    "    res_copy['user_id'] = res['user_id']\n",
    "    return res_copy\n",
    "\n",
    "def standartize_dataset(res, imputer=DATA_IMPUTER, transformer=DATA_TRANSFORMER):\n",
    "    from sklearn import impute, preprocessing\n",
    "\n",
    "    res_copy = res[res.columns.difference(['user_id'])]\n",
    "    res_copy = imputer.fit_transform(res_copy)\n",
    "    res_copy = transformer.fit_transform(res_copy)\n",
    "\n",
    "    return to_pandas_dataframe(res_copy, res), imputer, transformer\n",
    "\n",
    "def standartize_by_modifiers(res, imputer, transformer): \n",
    "    res_copy = res[res.columns.difference(['user_id'])]\n",
    "    res_copy = imputer.transform(res_copy)\n",
    "    res_copy = transformer.transform(res_copy)\n",
    "    return to_pandas_dataframe(res_copy, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Done!\n"
    }
   ],
   "source": [
    "n_features = 50\n",
    "df_train_mutated = mutate_dataset(df_train, transition_2=USE_TRANSITION_2, features=FEATURES[:n_features])\n",
    "train_y_mutated = mutate_dataset(train_y, transition_2=USE_TRANSITION_2,features=FEATURES[:n_features])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   A_N_mean    A_mean    B_mean    C_mean    D_mean  D_space_mean  E_N_mean  \\\n0  1.212049  0.709144  0.909885  0.636136  0.222051     -0.092068  1.027649   \n\n   E_R_mean    E_mean  E_space_mean  ...  shift_mean  space_A_mean  \\\n0  -0.26411  0.669065     -0.330727  ...   -0.228066     -0.344642   \n\n   space_I_mean  space_O_mean  space_S_mean  space_T_mean  space_W_mean  \\\n0     -0.262103      0.791729     -0.043113     -0.352727     -0.223755   \n\n   space_mean  space_shift_mean                user_id  \n0   -0.233073         -0.084365  A002160837SWJFPIAI7L7  \n\n[1 rows x 51 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A_N_mean</th>\n      <th>A_mean</th>\n      <th>B_mean</th>\n      <th>C_mean</th>\n      <th>D_mean</th>\n      <th>D_space_mean</th>\n      <th>E_N_mean</th>\n      <th>E_R_mean</th>\n      <th>E_mean</th>\n      <th>E_space_mean</th>\n      <th>...</th>\n      <th>shift_mean</th>\n      <th>space_A_mean</th>\n      <th>space_I_mean</th>\n      <th>space_O_mean</th>\n      <th>space_S_mean</th>\n      <th>space_T_mean</th>\n      <th>space_W_mean</th>\n      <th>space_mean</th>\n      <th>space_shift_mean</th>\n      <th>user_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.212049</td>\n      <td>0.709144</td>\n      <td>0.909885</td>\n      <td>0.636136</td>\n      <td>0.222051</td>\n      <td>-0.092068</td>\n      <td>1.027649</td>\n      <td>-0.26411</td>\n      <td>0.669065</td>\n      <td>-0.330727</td>\n      <td>...</td>\n      <td>-0.228066</td>\n      <td>-0.344642</td>\n      <td>-0.262103</td>\n      <td>0.791729</td>\n      <td>-0.043113</td>\n      <td>-0.352727</td>\n      <td>-0.223755</td>\n      <td>-0.233073</td>\n      <td>-0.084365</td>\n      <td>A002160837SWJFPIAI7L7</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 51 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "df_train_new, imputer, transformer = standartize_dataset(df_train_mutated)\n",
    "train_y_new = standartize_by_modifiers(train_y_mutated, imputer, transformer)\n",
    "\n",
    "train_y_new.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(832, 51)\nKNN Score: 0.6899038461538461\n(832, 24)\n(832, 24)\nKNN+PCA Score: 0.6021634615384616\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n                     weights='uniform')"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def knn_model(df, df_new, y_column): \n",
    "    knn = KNeighborsClassifier(n_neighbors=N_NEIGHBORS, p=DISTANCE_MEASURE)\n",
    "    knn.fit(df[df.columns.difference([y_column])], df[y_column])\n",
    "    score = knn.score(df_new[df_new.columns.difference([y_column])], df_new[y_column])\n",
    "    print(df.shape)\n",
    "    print(f\"KNN Score: {score}\")\n",
    "    return knn\n",
    "\n",
    "def knn_pca_model(df, df_new, y_column):\n",
    "    train_data = df[df.columns.difference([y_column])]\n",
    "    test_data = df_new[df_new.columns.difference([y_column])]\n",
    "    pca = PCA(n_components = 0.9).fit(train_data)\n",
    "    print(pca.transform(train_data).shape)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=N_NEIGHBORS, p=DISTANCE_MEASURE)\n",
    "    knn.fit(pca.transform(train_data), df[y_column])\n",
    "\n",
    "    print(pca.transform(test_data).shape)\n",
    "    score = knn.score(pca.transform(test_data), df_new[y_column])\n",
    "    print(f\"KNN+PCA Score: {score}\")\n",
    "    return knn, pca\n",
    "\n",
    "knn = knn_model(df_train_new, train_y_new, y_column='user_id')\n",
    "knn_pca, pca = knn_pca_model(df_train_new, train_y_new, y_column='user_id')\n",
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame(\n",
    "    (\n",
    "        (50, 0.6899038461538461),\n",
    "    ),\n",
    "    columns=['Features Number', 'Accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "176     A28M2RVUTQO891\n254     A2S16E0HN8A3N7\n268     A2W1E7UEQ7ZUQE\n289     A32SJS0TTSRIM5\n298     A34V3N0B4C3BMF\n342     A3GMI3KMQ4QOFQ\n412      AAXKOESYZ8Q86\n417      ABVBWRI4D5O36\n450      AK0RQYEM2D0AY\n488      AWYT0RI8P4YK0\n540     A2NTMYU9WLUDEO\n572     A3QAIABUJV1GAO\n618     A1R8LO8YXDJSMA\n624     A20IF612YY351J\n740     A2OXRSVXWXMR1R\n987      ANW7R2VV3V630\n993      ASSVV3K7SV9JT\n1209    A18RDBJFY3QWZL\nName: user_id, dtype: object"
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "res = knn.predict(train_y_new[train_y_new.columns.difference(['user_id'])])\n",
    "_rev = train_y_new.copy() \n",
    "_rev['predicted_user_id'] = res\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "_rev[_rev.user_id != _rev.predicted_user_id]['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "41"
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "len(_rev[_rev.user_id != _rev.predicted_user_id]['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1071"
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "source": [
    "len(train_y_new.user_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f5e52c3d7aff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFEATURES_COUNTER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'counter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.sort(reverse=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# pd.set_option('display.max_rows', 200)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# _a.sort_values(by=['counter'], ascending=False).head(200)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "_a=pd.DataFrame(list(FEATURES_COUNTER.items()), columns=['key', 'counter'])#.sort(reverse=True)\n",
    "# pd.set_option('display.max_rows', 200)\n",
    "# _a.sort_values(by=['counter'], ascending=False).head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'dict_items' object has no attribute '__reversed__'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-02d26e713156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFEATURES_COUNTER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__reversed__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict_items' object has no attribute '__reversed__'"
     ]
    }
   ],
   "source": [
    "sorted(list(FEATURES_COUNTER.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "\n<div id=\"altair-viz-9169b8924a7c437f893742d1e7f5fdbb\"></div>\n<script type=\"text/javascript\">\n  (function(spec, embedOpt){\n    const outputDiv = document.getElementById(\"altair-viz-9169b8924a7c437f893742d1e7f5fdbb\");\n    const paths = {\n      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n    };\n\n    function loadScript(lib) {\n      return new Promise(function(resolve, reject) {\n        var s = document.createElement('script');\n        s.src = paths[lib];\n        s.async = true;\n        s.onload = () => resolve(paths[lib]);\n        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n        document.getElementsByTagName(\"head\")[0].appendChild(s);\n      });\n    }\n\n    function showError(err) {\n      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n      throw err;\n    }\n\n    function displayChart(vegaEmbed) {\n      vegaEmbed(outputDiv, spec, embedOpt)\n        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n    }\n\n    if(typeof define === \"function\" && define.amd) {\n      requirejs.config({paths});\n      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n    } else if (typeof vegaEmbed === \"function\") {\n      displayChart(vegaEmbed);\n    } else {\n      loadScript(\"vega\")\n        .then(() => loadScript(\"vega-lite\"))\n        .then(() => loadScript(\"vega-embed\"))\n        .catch(showError)\n        .then(() => displayChart(vegaEmbed));\n    }\n  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"url\": \"tmp/altdata/altair-data-8f31a7b89e7cbc6279e61580590041b8.json\", \"format\": {\"type\": \"json\"}}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"counter\"}, \"y\": {\"type\": \"ordinal\", \"field\": \"key\", \"sort\": {\"field\": \"counter\", \"order\": \"descending\"}}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\"}, {\"mode\": \"vega-lite\"});\n</script>",
      "text/plain": "alt.Chart(...)"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "alt.Chart(_a.sort_values(by=['counter'], ascending=False).head(20)).mark_bar().encode(\n",
    " x = 'counter:Q',\n",
    " y = alt.Y('key:O', sort = alt.Sort(field = 'counter', order='descending')),\n",
    ")#.transform_filter('datum.counter > 2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['space', 'E', 'T', 'A', 'O', 'backspace', 'N', 'I', 'S', 'R', 'H',\n       'backspace_backspace', 'L', 'E_space', 'D', 'space_T', 'U', 'C',\n       'space_A', 'T_H', 'S_space', 'M', 'shift', 'G', 'Y', 'T_space',\n       'F', 'H_E', 'W', 'P', 'D_space', 'A_N', 'I_N', 'R_E', 'N_space',\n       'Y_space', 'E_R', 'B', 'space_shift', 'space_I', 'space_O',\n       'space_W', 'V', 'dot', 'space_S', 'O_space', 'O_N', 'N_D', 'E_N',\n       'R_space', 'dot_space', 'A_T', 'O_U', 'T_O', 'space_C', 'I_T',\n       'space_B', 'A_R', 'E_S', 'H_A', 'I_S', 'V_E', 'comma', 'A_L',\n       'space_M', 'space_F', 'comma_space', 'R_I', 'O_R',\n       'space_backspace', 'L_E', 'S_T', 'N_T', 'N_G', 'space_P',\n       'space_H', 'S_E', 'T_I', 'A_space', 'M_E', 'space_G', 'T_E', 'E_D',\n       'F_space', 'L_space', 'E_A', 'K', 'A_S', 'B_E', 'M_A', 'L_L',\n       'G_space', '[shift]space', '[shift]I', 'space_R', 'space_D', 'U_N',\n       'O_F', 'H_O', 'H_I', 'C_O', 'shift_I', 'N_E', 'space_L', 'R_O',\n       'space_N', 'O_M', 'O_T', 'L_I', 'space_E', 'D_E', '[shift]T',\n       'N_S', 'I_C', 'C_E', 'L_Y', 'T_A', 'G_E', 'I_O', 'U_R', 'E_L',\n       'F_O', 'U_S', 'N_O', 'P_E', 'H_space', 'S_O', 'I_L', 'W_E',\n       'shift_T', 'C_H', 'C_A', 'P_L', 'U_L', 'E_T', 'W_A', 'A_Y', 'T_R',\n       'L_D', 'U_T', 'L_A', 'R_A', 'E_C', 'E_E', 'I_E', 'W_I',\n       '[shift]space_shift', 'backspace_space', 'G_U', 'W_H', 'S_H',\n       'A_V', 'A_C', 'E_backspace', 'O_O', 'space_space', 'O_P', 'R_Y',\n       'O_W', 'I_A', 'R_R', 'Þ', 'L_O', 'S_I', 'O_L', 'A_G', 'D_I', 'I_G',\n       'E_V', 'A_M', 'G_H', 'W_O', 'Y_O', 'M_O', 'backspace_shift', 'O_S',\n       'G_A', 'P_O', 'K_E', 'R_S', 'backspace_E', 'P_R', 'S_dot', 'E_dot',\n       'I_M', 'I_R', 'M_space', 'S_A', 'backspace_T', 'backspace_A', 'X',\n       'C_I', 'space_Y', 'E_Y', 'F_E', 'O_D', 'D_O', 'space_U', 'T_S',\n       'A_D'], dtype=object)"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "_a.sort_values(by=['counter'], ascending=False).head(200).key.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}